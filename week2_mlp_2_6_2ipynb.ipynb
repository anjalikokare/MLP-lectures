{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0E0kjP69P9IQrje9A2G8A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjalikokare/MLP-lectures/blob/main/week2_mlp_2_6_2ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **feature scalling**"
      ],
      "metadata": {
        "id": "mHmZ0WebORf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "* **machine learning models need clean, preprocesses data.**\n",
        "* **features often have different scales(e.g, age vs.income)**\n",
        "* **this can cause models to misintercept feature importance**\n",
        "* feature scalling puts all features on similar scale for better performance.\n",
        "\n",
        "* **Feature scalling** : Its a way adjust numerical data to that all features have a similar range\n"
      ],
      "metadata": {
        "id": "quFkoutWO5Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler"
      ],
      "metadata": {
        "id": "s5aZrRpiSWgi"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "house_price = [\n",
        "    {'age':10, 'area':6070, 'bedrooms':3,'stories': 3, 'belcony': 1, 'price(in M)': 5.0},\n",
        "    {'age':20, 'area':7888, 'bedrooms':4,'stories': 2, 'belcony': 3, 'price(in M)': 7.0},\n",
        "    {'age':20, 'area':8925, 'bedrooms':4,'stories': 2, 'belcony': 3, 'price(in M)': 7.3},\n",
        "    {'age':30, 'area':8700,'bedrooms':2, 'stories': 4, 'belcony': 2, 'price(in M)': 6.0},\n",
        "    {'age':40, 'area':8000,'bedrooms':5, 'stories': 4, 'belcony': 4, 'price(in M)': 5.0},\n",
        "    {'age':40, 'area':7520, 'bedrooms':3,'stories': 4,'belcony': 1, 'price(in M)': 6.0}\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "K2EvTzxLQO2q"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dp = pd.DataFrame(house_price)"
      ],
      "metadata": {
        "id": "6sskSWUSSRxK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOOxmxqiSZ5r",
        "outputId": "4a1b8a35-2c66-492b-bff2-742729b1b8c9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age  area  bedrooms  stories  belcony  price(in M)\n",
            "0   10  6070         3        3        1          5.0\n",
            "1   20  7888         4        2        3          7.0\n",
            "2   20  8925         4        2        3          7.3\n",
            "3   30  8700         2        4        2          6.0\n",
            "4   40  8000         5        4        4          5.0\n",
            "5   40  7520         3        4        1          6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc = StandardScaler()\n",
        "sc.fit_transform(dp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MAaOVEKUHV6",
        "outputId": "87cc0a45-be0d-49f1-da61-033cd7368bd4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.50755672, -1.91636945, -0.52223297, -0.18569534, -1.20604538,\n",
              "        -1.18952649],\n",
              "       [-0.60302269,  0.04036161,  0.52223297, -1.29986737,  0.60302269,\n",
              "         1.07623825],\n",
              "       [-0.60302269,  1.15649479,  0.52223297, -1.29986737,  0.60302269,\n",
              "         1.41610296],\n",
              "       [ 0.30151134,  0.91432511, -1.5666989 ,  0.92847669, -0.30151134,\n",
              "        -0.05664412],\n",
              "       [ 1.20604538,  0.1609083 ,  1.5666989 ,  0.92847669,  1.50755672,\n",
              "        -1.18952649],\n",
              "       [ 1.20604538, -0.35572036, -0.52223297,  0.92847669, -1.20604538,\n",
              "        -0.05664412]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Scallling advantages**\n",
        "\n",
        "* feature scalling makes numeical features camparable in scale\n",
        "* it prevent models from favoring features with larger values\n",
        "* without it, algortihm may converge slowly or perform poorly\n",
        "\n"
      ],
      "metadata": {
        "id": "IRlcFZ_AVMDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# * **Types of feature scallling**\n",
        "\n",
        "* **standardization(z-score Normalization)**\n",
        "$$x' = \\frac{x_i - \\mu}{\\sigma}$$\n",
        "\n",
        "* recommended for: Improve model performance by making features resemble standard normal data\n",
        "\n",
        "* **Min-Max_scaler**\n",
        "* $x' = \\frac{x - x.mean}{x.max - x.min}$\n",
        "* range from [0,1] for minmaxscaler\n",
        "* standardScaler and minMaxScaler very sensitive to the persence of the outlier"
      ],
      "metadata": {
        "id": "R2dAnx11WV7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    {'feature1':1 , 'feature32':50, 'feature3':200},\n",
        "    {'feature1':2 , 'feature32':60, 'feature3':180},\n",
        "    {'feature1':3 , 'feature32':70, 'feature3':160},\n",
        "    {'feature1':4 , 'feature32':80, 'feature3':140},\n",
        "    {'feature1':5 , 'feature32':90, 'feature3':120}\n",
        "]"
      ],
      "metadata": {
        "id": "3VNuSegIi_ss"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGRA2RGEkMrd",
        "outputId": "936e7ed6-498d-4fc7-bc4f-b7dfa1727663"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   feature1  feature32  feature3\n",
            "0         1         50       200\n",
            "1         2         60       180\n",
            "2         3         70       160\n",
            "3         4         80       140\n",
            "4         5         90       120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ss = StandardScaler()\n",
        "ss.fit_transform(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4vmzjupj3Yx",
        "outputId": "c6c34e5a-cc4b-404e-8c86-fc190f2bd181"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.41421356, -1.41421356,  1.41421356],\n",
              "       [-0.70710678, -0.70710678,  0.70710678],\n",
              "       [ 0.        ,  0.        ,  0.        ],\n",
              "       [ 0.70710678,  0.70710678, -0.70710678],\n",
              "       [ 1.41421356,  1.41421356, -1.41421356]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mms = MinMaxScaler()\n",
        "mms.fit_transform(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsmWSC6KknFt",
        "outputId": "d0401b28-24b3-4052-f0b9-cfb1513bd406"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 1.  ],\n",
              "       [0.25, 0.25, 0.75],\n",
              "       [0.5 , 0.5 , 0.5 ],\n",
              "       [0.75, 0.75, 0.25],\n",
              "       [1.  , 1.  , 0.  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Robust Scaling(median and IQR- Based)**\n",
        "\n",
        "* formula = $x' = \\frac{x - Q(x)}{Q3(x) - Q1(X)}$\n",
        "* Q2 = median of the dataset\n",
        "* Q3 = 75% of the dataset\n",
        "* Q1 = 25% of the dataset"
      ],
      "metadata": {
        "id": "6WzpXc2Smv_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rs = RobustScaler()\n",
        "rs.fit_transform(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZBmP5vKnvv-",
        "outputId": "b604fae3-5d3b-411c-e623-2779403e59f8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1. , -1. ,  1. ],\n",
              "       [-0.5, -0.5,  0.5],\n",
              "       [ 0. ,  0. ,  0. ],\n",
              "       [ 0.5,  0.5, -0.5],\n",
              "       [ 1. ,  1. , -1. ]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Max Abs Scalling**\n",
        "\n",
        "formula = $x' = \\frac{x}{max|x|}$\n",
        "\n",
        "* recommended for : Best for sparse data"
      ],
      "metadata": {
        "id": "j-PPZ_Pmoh81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mas = MaxAbsScaler()\n",
        "mas.fit_transform(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3zrbmfSo_o7",
        "outputId": "b58e263f-30aa-49c8-d330-d79f52294784"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.2       , 0.55555556, 1.        ],\n",
              "       [0.4       , 0.66666667, 0.9       ],\n",
              "       [0.6       , 0.77777778, 0.8       ],\n",
              "       [0.8       , 0.88888889, 0.7       ],\n",
              "       [1.        , 1.        , 0.6       ]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Distance-based Models**\n",
        "* **K-Nearest Neighbours(KNN)** --> distance-based algorithm(Euclidean Distance) so numbers must in quality.\n",
        "\n",
        "* **Support Vector machine(SVM)** --> SVMs rely on distance, so scalling boosts performance-especially with RBF Kernel\n"
      ],
      "metadata": {
        "id": "2k-nds-zp03f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dimentionality Reducation Technique**\n",
        "\n",
        "* **Principal Component Analysis(PCA)** --> because the varience is high for high-magnitude features so scalling is critical.\n",
        ""
      ],
      "metadata": {
        "id": "8KoYeto9q4-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradient-Based Models**\n",
        "* To ensure smooth convergence of gradient descent and consistent update rates acress all features\n",
        "E.g Logistic Regression"
      ],
      "metadata": {
        "id": "JZe78qtIrbjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tree-based Models (Minimal Impact)**\n",
        "\n",
        "* classification and regression trees(CART)\n",
        "* random forest\n"
      ],
      "metadata": {
        "id": "KFfArmDAyOre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Key Takeaways**\n",
        "* Feature scaling is an important preprocessing step, but it doesn't always improve performance.\n",
        "\n",
        "* Scaling is crucial for models that rely on distance calculations, like SVM, KNN, and Logistic Regression.\n",
        "\n",
        "* Tree-based models (e.g., Decision Trees, Random Forest) are mostly unaffected by scaling.\n",
        "\n",
        "* Avoid data leakage by fitting the scaler only on training data and applying the transformation to test data separately.\n",
        "\n",
        "* Different scalers (StandardScaler, MinMaxScaler, etc.) can impact model performance differently, so it's important to choose the right one."
      ],
      "metadata": {
        "id": "bMhtcoXY0QN3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pBZHBzJg0aUX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}